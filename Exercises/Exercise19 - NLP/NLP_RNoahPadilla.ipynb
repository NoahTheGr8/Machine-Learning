{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_RNoahPadilla.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccVy8FvS8w1K",
        "outputId": "e8261a61-6755-4c5c-be97-763c4f7864c8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7wyHVvu5WP8"
      },
      "source": [
        "Get corpus from URL's > Word2vec each word in each sentence from each of the 3 books > Create X and y data from it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9mASvyhiZEG"
      },
      "source": [
        "'''\n",
        "Author: Olac Fuentes\n",
        "Modified by: R Noah Padilla\n",
        "\n",
        "The goal of this assignment is written below.\n",
        "\n",
        "The program read_sentences.py reads sentences form online classic books and \n",
        "converts them to a list of sentences, where each sentence is a list of words.\n",
        "\n",
        "    [x]1. Write a function that receives a sentence and returns a 2D array containing \n",
        "        the embeddings of the words in the sentence. Your function should receive the embeddings \n",
        "        dictionary, the sentence and the desired length of the representation; if the \n",
        "        sentence is shorter than the desired length, path the array with zeros; if itâ€™s longer, \n",
        "        truncate the representation.\n",
        "    \n",
        "    [x]2. Apply the function to produce an embedding representation of each of the \n",
        "        sentences in the three books used in the read_sentences.py program and generate\n",
        "        a dataset containing examples of 3 classes, one for each book.\n",
        "        \n",
        "                > apply function to each sentence from each book and save all of them into a data set X and y\n",
        "    \n",
        "    [x]3. Randomly split the data into training and testing.\n",
        "    \n",
        "    [x]4. Train and test a system to determine the book each sentence belongs to.\n",
        "    \n",
        "    CLASSIFICATION PROBLEM\n",
        "\n",
        "'''\n",
        "\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "'''\n",
        "TODO\n",
        "sent_embedder():\n",
        "        - receives a sentence, word embeddings dictionary, and the desired length of the sentence represention\n",
        "        - returns a 2D array containing each of the words embeddings for a given sentence| row = word and columns are the embedding values\n",
        "\n",
        "'''\n",
        "def sent_embedder(sent, emb, desLen):\n",
        "    \n",
        "    #>>> Trim/Truncate sentences based on 'uSeRs' desires\n",
        "    if len(sent) < desLen:\n",
        "        extraZeros = [0]*(desLen - len(sent))#add zeros instead\n",
        "        extraZeros = [str(x) for x in extraZeros] \n",
        "        sent.extend(extraZeros)\n",
        "    elif len(sent) > desLen:    #truncate\n",
        "        sent = sent[:desLen]\n",
        "    \n",
        "    #>>> Get embeddings for each word | word = row , col = emb values\n",
        "    sent_emb = [] #contains all a word embeddings for each word in 'sent' that we will return\n",
        "    for word in sent:\n",
        "        if emb.get(word) is None:\n",
        "          sent_emb.append(emb.get(str(0)))\n",
        "        else:\n",
        "          sent_emb.append(emb.get(word))\n",
        "    return sent_emb\n",
        "\n",
        "def read_embeddings(n=1000):\n",
        "    #Fuentes: Reads n embeddings from file\n",
        "    #Fuentes: Returns a dictionary were embedding[w] is the embeding of string w\n",
        "    embedding = {}\n",
        "    count = 0\n",
        "\n",
        "    #change line 69 to your drive and make sure you have the glove file in your drive\n",
        "    with open('/content/drive/MyDrive/MachineLearningData/glove.6B.50d.txt', encoding=\"utf8\") as f: \n",
        "        for line in f: \n",
        "            count+=1\n",
        "            ls = line.split(\" \")\n",
        "            emb = [np.float32(x) for x in ls[1:]]\n",
        "            embedding[ls[0]]=np.array(emb)\n",
        "            if count>= n:\n",
        "                break\n",
        "    return embedding\n",
        "\n",
        "def get_words(st):\n",
        "    st = st.lower()\n",
        "    st = st.replace('\\r\\n', ' ')\n",
        "    st = ''.join( c for c in st if  c in lowercase)\n",
        "    words = st.split()\n",
        "    return words\n",
        "\n",
        "def get_sentence_list(url):\n",
        "    paragraphs = []\n",
        "    word_lists = []\n",
        "    sentence_list = []\n",
        "    data = urllib.request.urlopen(url).read()\n",
        "    soup = bs.BeautifulSoup(data,'lxml')\n",
        "    count = 0\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        par  = paragraph.string\n",
        "        if par:\n",
        "            par = par.replace('\\r\\n', ' ')\n",
        "            sent = par.split('.')\n",
        "            for s in sent:\n",
        "                sentence_list.append(s+'.')         \n",
        "                words = get_words(s)\n",
        "                if len(words)>0:\n",
        "                    word_lists.append(words)\n",
        "    return word_lists\n",
        "\n",
        "if __name__ == \"__main__\":  \n",
        "    url_list = ['http://www.gutenberg.org/files/215/215-h/215-h.htm', 'http://www.gutenberg.org/files/345/345-h/345-h.htm', 'http://www.gutenberg.org/files/1661/1661-h/1661-h.htm']\n",
        "    lowercase = ''.join(chr(i) for i in range(97,123)) + ' '       \n",
        "    \n",
        "    allSentences = [] #contains all the sentences or word lists from all 3 books combined\n",
        "    numSentEachBook = [] # number of sentences in each book | index 0 = num sentences in book 0\n",
        "    for u, url in enumerate(url_list):\n",
        "        word_lists = get_sentence_list(url)\n",
        "        print('Book {} contains {} sentences'.format(u,len(word_lists)))\n",
        "        lengths = np.array([len(wl) for wl in word_lists])\n",
        "        print('Sentence length stats (min,max and mean words in a sentence):')\n",
        "        print('min = {} max = {} mean = {:4f}'.format(np.min(lengths),np.max(lengths),np.mean(lengths)))\n",
        "        numSentEachBook.append(len(lengths)) #len(lengths) = total number of sentences for book 'u'\n",
        "        allSentences.extend(word_lists)\n",
        "        \n",
        "    print('Total number of sentences in all 3 books: ', len(allSentences))\n",
        "    \n",
        "    vocabulary_size = 22500        \n",
        "    embedding = read_embeddings(vocabulary_size)\n",
        "    '''\n",
        "    #Fuentes: See if the protagonists appear in the embedding list    \n",
        "    #Fuentes: I recommend increasing vocabulary size until all 3 appear in vocabulary\n",
        "    for w in ['buck','dracula','holmes']:\n",
        "        try:\n",
        "            print(w,'embedding:\\n',embedding[w])\n",
        "        except:\n",
        "            print(w,'is not in dictionary')\n",
        "            pass\n",
        "    '''\n",
        "\n",
        "    #Regarding y -> Each sentence is mapped to a book > 2D array mapped is mapped to a number ranging from 1-3\n",
        "    \n",
        "    #get each word embedding from each book - contains duplicated embeddings\n",
        "    desiredLength = 7\n",
        "    all_word_emb = [] #should be a list of 2D arrays where each 2D array is a sentence embedding (represents the Word2vec)\n",
        "    for sent in allSentences:\n",
        "        all_word_emb.append(sent_embedder(sent, embedding,desiredLength))\n",
        "\n",
        "    print(\"Total sentence embeddings calculated: \",len(all_word_emb)) #should be 12260 bc thats how many sentences are in all 3 books combined -\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVPCAi-p-mPZ"
      },
      "source": [
        "Seperate the data into X and y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ziat2J9z-sqp"
      },
      "source": [
        "X = all_word_emb\n",
        "y = [] # create a one hot rep of the data | [1,0,0] means book 0, [0,1,0] means book 1, [0,0,1] means book 2\n",
        "\n",
        "#numSentEachBook is a list where index 0(book zero) contains number of sentences for that book and so on\n",
        "for book in range(len(numSentEachBook)):\n",
        "  for sent in range(numSentEachBook[book]):\n",
        "    ohRep = np.zeros(3) #3 bc there are 3 classes | book 0,1,2\n",
        "    ohRep[book] = 1\n",
        "    y.append(ohRep)\n",
        "\n",
        "#print(len(X)) # used for debugging\n",
        "#print(len(y)) # used for debugging"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd7ggpW7PWB2"
      },
      "source": [
        "Split into training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dIyyUucPZRq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "'''\n",
        "#USED FOR DEBUGGING\n",
        "print(X.shape)\n",
        "print(type(X))\n",
        "print(y.shape)\n",
        "print(type(y))\n",
        "'''\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2020)\n",
        "\n",
        "'''\n",
        "#USED FOR DEBUGGING\n",
        "print(X_train.shape)\n",
        "print(type(X_train))\n",
        "print(y_train.shape)\n",
        "print(type(y_train))\n",
        "\n",
        "print(X_test.shape)\n",
        "print(type(X_test))\n",
        "print(y_test.shape)\n",
        "print(type(y_test))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXc-GcRRQfAx"
      },
      "source": [
        "Use a CNN or RNN to classify what book a given sentence belongs to"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w4kbOTiR1jy"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, SGD\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import distutils\n",
        "\n",
        "'''\n",
        "With help from - https://www.tensorflow.org/guide/keras/rnn\n",
        "'''\n",
        "\n",
        "def create_model():\n",
        " \n",
        "  #Leave this line alone - used in all models\n",
        "  model = tf.keras.models.Sequential()\n",
        "  \n",
        "  '''\n",
        "  #1D CNN-----------------------------------------------------------------------------------------------------------------------\n",
        "  model.add(tf.keras.layers.Conv1D(32, 3, input_shape=(X_train.shape[1],X_train.shape[2]), padding=\"valid\" ,  activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv1D(64, 3, input_shape=(X_train.shape[1],X_train.shape[2]),  padding=\"valid\" , activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
        "\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(64,activation ='relu'))\n",
        "  model.add(tf.keras.layers.Dense(3,activation = 'softmax'))\n",
        "  \n",
        "\n",
        "  #Simple RNN--------------------------------------------------------------------------------------------------------------------\n",
        "  model.add(tf.keras.layers.SimpleRNN(64, activation = 'relu', input_shape=(X_train.shape[1],X_train.shape[2])))\n",
        "\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(64,activation ='relu'))\n",
        "  model.add(tf.keras.layers.Dense(3,activation = 'softmax'))  \n",
        "  \n",
        "  #GRU---------------------------------------------------------------------------------------------------------------------------\n",
        "  model.add(tf.keras.layers.GRU(64, activation = 'relu', input_shape=(X_train.shape[1],X_train.shape[2])))\n",
        "\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(64,activation ='relu'))\n",
        "  model.add(tf.keras.layers.Dense(3,activation = 'softmax'))  \n",
        "  '''\n",
        "  #LSTM--------------------------------------------------------------------------------------------------------------------------\n",
        "  model.add(tf.keras.layers.LSTM(64, activation = 'relu', input_shape=(X_train.shape[1],X_train.shape[2])))\n",
        "\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(64,activation ='relu'))\n",
        "  model.add(tf.keras.layers.Dense(3,activation = 'softmax'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI3hhs3FR_LA"
      },
      "source": [
        "model = create_model()\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=50, epochs=20, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYZ52-5z8Lod"
      },
      "source": [
        "# **RESULTS** \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BHSbJ0y7xrC"
      },
      "source": [
        "\n",
        "<<<USING 1D CONV with 2 Layers>>>\n",
        "\n",
        "        Epoch 15/20\n",
        "        184/184 [==============================] - 1s 4ms/step - loss: 0.4062 - accuracy: 0.8293 - val_loss: 0.9724 - val_accuracy: 0.5984\n",
        "        Epoch 16/20\n",
        "        184/184 [==============================] - 1s 3ms/step - loss: 0.3809 - accuracy: 0.8438 - val_loss: 1.0350 - val_accuracy: 0.6000\n",
        "        Epoch 17/20\n",
        "        184/184 [==============================] - 1s 3ms/step - loss: 0.3659 - accuracy: 0.8527 - val_loss: 1.0541 - val_accuracy: 0.6013\n",
        "        Epoch 18/20\n",
        "        184/184 [==============================] - 1s 3ms/step - loss: 0.3406 - accuracy: 0.8656 - val_loss: 1.1419 - val_accuracy: 0.6072\n",
        "        Epoch 19/20\n",
        "        184/184 [==============================] - 1s 3ms/step - loss: 0.3254 - accuracy: 0.8705 - val_loss: 1.1757 - val_accuracy: 0.5935\n",
        "        Epoch 20/20\n",
        "        184/184 [==============================] - 1s 3ms/step - loss: 0.3120 - accuracy: 0.8784 - val_loss: 1.1888 - val_accuracy: 0.5938\n",
        "\n",
        "<<< USING Simple RNN >>>\n",
        "\n",
        "        Epoch 15/20\n",
        "        184/184 [==============================] - 1s 7ms/step - loss: 0.5407 - accuracy: 0.7525 - val_loss: 0.8520 - val_accuracy: 0.6072\n",
        "        Epoch 16/20\n",
        "        184/184 [==============================] - 1s 7ms/step - loss: 0.5225 - accuracy: 0.7637 - val_loss: 0.8829 - val_accuracy: 0.6134\n",
        "        Epoch 17/20\n",
        "        184/184 [==============================] - 1s 8ms/step - loss: 0.4975 - accuracy: 0.7775 - val_loss: 0.8968 - val_accuracy: 0.6209\n",
        "        Epoch 18/20\n",
        "        184/184 [==============================] - 1s 8ms/step - loss: 0.4830 - accuracy: 0.7866 - val_loss: 0.9265 - val_accuracy: 0.6101\n",
        "        Epoch 19/20\n",
        "        184/184 [==============================] - 1s 7ms/step - loss: 0.4665 - accuracy: 0.7928 - val_loss: 0.9928 - val_accuracy: 0.6254\n",
        "        Epoch 20/20\n",
        "        184/184 [==============================] - 1s 7ms/step - loss: 0.4448 - accuracy: 0.8070 - val_loss: 0.9973 - val_accuracy: 0.6003\n",
        "\n",
        "<<< USING GRU >>> \n",
        "\n",
        "        Epoch 15/20\n",
        "        184/184 [==============================] - 3s 17ms/step - loss: 0.4853 - accuracy: 0.7804 - val_loss: 0.8100 - val_accuracy: 0.6545\n",
        "        Epoch 16/20\n",
        "        184/184 [==============================] - 3s 18ms/step - loss: 0.4792 - accuracy: 0.7852 - val_loss: 0.8142 - val_accuracy: 0.6535\n",
        "        Epoch 17/20\n",
        "        184/184 [==============================] - 3s 17ms/step - loss: 0.4599 - accuracy: 0.7923 - val_loss: 0.8169 - val_accuracy: 0.6617\n",
        "        Epoch 18/20\n",
        "        184/184 [==============================] - 3s 19ms/step - loss: 0.4437 - accuracy: 0.8075 - val_loss: 0.8167 - val_accuracy: 0.6444\n",
        "        Epoch 19/20\n",
        "        184/184 [==============================] - 3s 17ms/step - loss: 0.4198 - accuracy: 0.8132 - val_loss: 0.8552 - val_accuracy: 0.6623\n",
        "        Epoch 20/20\n",
        "        184/184 [==============================] - 3s 17ms/step - loss: 0.4023 - accuracy: 0.8282 - val_loss: 0.8730 - val_accuracy: 0.6512\n",
        "\n",
        "<<< USING LSTM >>> *** BEST RESULTS ***\n",
        "\n",
        "        Epoch 15/20\n",
        "        184/184 [==============================] - 3s 14ms/step - loss: 0.4645 - accuracy: 0.7938 - val_loss: 0.7948 - val_accuracy: 0.6515\n",
        "        Epoch 16/20\n",
        "        184/184 [==============================] - 3s 14ms/step - loss: 0.4477 - accuracy: 0.7983 - val_loss: 0.8253 - val_accuracy: 0.6626\n",
        "        Epoch 17/20\n",
        "        184/184 [==============================] - 3s 17ms/step - loss: 0.4267 - accuracy: 0.8146 - val_loss: 0.8601 - val_accuracy: 0.6532\n",
        "        Epoch 18/20\n",
        "        184/184 [==============================] - 3s 15ms/step - loss: 0.4138 - accuracy: 0.8173 - val_loss: 0.8475 - val_accuracy: 0.6623\n",
        "        Epoch 19/20\n",
        "        184/184 [==============================] - 3s 17ms/step - loss: 0.3932 - accuracy: 0.8307 - val_loss: 0.9127 - val_accuracy: 0.6584\n",
        "        Epoch 20/20\n",
        "        184/184 [==============================] - 2s 14ms/step - loss: 0.3783 - accuracy: 0.8353 - val_loss: 0.9540 - val_accuracy: 0.6440\n"
      ]
    }
  ]
}