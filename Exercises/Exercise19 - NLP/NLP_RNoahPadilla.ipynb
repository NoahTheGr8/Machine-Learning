{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_RNoahPadilla.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccVy8FvS8w1K",
        "outputId": "b268e5d8-eb74-478a-aba3-93456d5ee263"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7wyHVvu5WP8"
      },
      "source": [
        "Get corpus from URL's > Word2vec each word in each sentence from each of the 3 books > Create X and y data from it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9mASvyhiZEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce1f700-04bf-442b-daa5-3f76ec8c8dbe"
      },
      "source": [
        "'''\n",
        "Author: Olac Fuentes\n",
        "Modified by: R Noah Padilla\n",
        "\n",
        "The goal of this assignment is written below.\n",
        "\n",
        "The program read_sentences.py reads sentences form online classic books and \n",
        "converts them to a list of sentences, where each sentence is a list of words.\n",
        "    \n",
        "\n",
        "\n",
        "    [x]1. Write a function that receives a sentence and returns a 2D array containing \n",
        "        the embeddings of the words in the sentence. Your function should receive the embeddings \n",
        "        dictionary, the sentence and the desired length of the representation; if the \n",
        "        sentence is shorter than the desired length, path the array with zeros; if itâ€™s longer, \n",
        "        truncate the representation.\n",
        "    \n",
        "    [x]2. Apply the function to produce an embedding representation of each of the \n",
        "        sentences in the three books used in the read_sentences.py program and generate\n",
        "        a dataset containing examples of 3 classes, one for each book.\n",
        "        \n",
        "                > apply function to each sentence from each book and save all of them into a data set X and y\n",
        "    \n",
        "    [x]3. Randomly split the data into training and testing.\n",
        "    \n",
        "    [x]4. Train and test a system to determine the book each sentence belongs to.\n",
        "    \n",
        "    CLASSIFICATION PROBLEM\n",
        "\n",
        "'''\n",
        "\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "'''\n",
        "TODO\n",
        "sent_embedder():\n",
        "        - receives a sentence, word embeddings dictionary, and the desired length of the sentence represention\n",
        "        - returns a 2D array containing each of the words embeddings for a given sentence| row = word and columns are the embedding values\n",
        "\n",
        "'''\n",
        "def sent_embedder(sent, emb, desLen):\n",
        "    \n",
        "    #>>> Trim/Truncate sentences based on 'uSeRs' desires\n",
        "    if len(sent) < desLen:\n",
        "        extraZeros = [0]*(desLen - len(sent))#add zeros instead\n",
        "        extraZeros = [str(x) for x in extraZeros] \n",
        "        sent.extend(extraZeros)\n",
        "    elif len(sent) > desLen:    #truncate\n",
        "        sent = sent[:desLen]\n",
        "    \n",
        "    #>>> Get embeddings for each word | word = row , col = emb values\n",
        "    sent_emb = [] #contains all a word embeddings for each word in 'sent' that we will return\n",
        "    for word in sent:\n",
        "        if emb.get(word) is None:\n",
        "          sent_emb.append(emb.get(str(0)))\n",
        "        else:\n",
        "          sent_emb.append(emb.get(word))\n",
        "    return sent_emb\n",
        "\n",
        "def read_embeddings(n=1000):\n",
        "    #Fuentes: Reads n embeddings from file\n",
        "    #Fuentes: Returns a dictionary were embedding[w] is the embeding of string w\n",
        "    embedding = {}\n",
        "    count = 0\n",
        "    with open('/content/drive/MyDrive/MachineLearningData/glove.6B.50d.txt', encoding=\"utf8\") as f: \n",
        "        for line in f: \n",
        "            count+=1\n",
        "            ls = line.split(\" \")\n",
        "            emb = [np.float32(x) for x in ls[1:]]\n",
        "            embedding[ls[0]]=np.array(emb)\n",
        "            if count>= n:\n",
        "                break\n",
        "    return embedding\n",
        "\n",
        "def get_words(st):\n",
        "    st = st.lower()\n",
        "    st = st.replace('\\r\\n', ' ')\n",
        "    st = ''.join( c for c in st if  c in lowercase)\n",
        "    words = st.split()\n",
        "    return words\n",
        "\n",
        "def get_sentence_list(url):\n",
        "    paragraphs = []\n",
        "    word_lists = []\n",
        "    sentence_list = []\n",
        "    data = urllib.request.urlopen(url).read()\n",
        "    soup = bs.BeautifulSoup(data,'lxml')\n",
        "    count = 0\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        par  = paragraph.string\n",
        "        if par:\n",
        "            par = par.replace('\\r\\n', ' ')\n",
        "            sent = par.split('.')\n",
        "            for s in sent:\n",
        "                sentence_list.append(s+'.')         \n",
        "                words = get_words(s)\n",
        "                if len(words)>0:\n",
        "                    word_lists.append(words)\n",
        "    return word_lists\n",
        "\n",
        "if __name__ == \"__main__\":  \n",
        "    url_list = ['http://www.gutenberg.org/files/215/215-h/215-h.htm', 'http://www.gutenberg.org/files/345/345-h/345-h.htm', 'http://www.gutenberg.org/files/1661/1661-h/1661-h.htm']\n",
        "    lowercase = ''.join(chr(i) for i in range(97,123)) + ' '       \n",
        "    \n",
        "    allSentences = [] #contains all the sentences or word lists from all 3 books combined\n",
        "    numSentEachBook = [] # number of sentences in each book | index 0 = num sentences in book 0\n",
        "    for u, url in enumerate(url_list):\n",
        "        word_lists = get_sentence_list(url)\n",
        "        print('Book {} contains {} sentences'.format(u,len(word_lists)))\n",
        "        lengths = np.array([len(wl) for wl in word_lists])\n",
        "        print('Sentence length stats (min,max and mean words in a sentence):')\n",
        "        print('min = {} max = {} mean = {:4f}'.format(np.min(lengths),np.max(lengths),np.mean(lengths)))\n",
        "        numSentEachBook.append(len(lengths)) #len(lengths) = total number of sentences for book 'u'\n",
        "        allSentences.extend(word_lists)\n",
        "        \n",
        "    print('Total number of sentences in all 3 books: ', len(allSentences))\n",
        "    \n",
        "    vocabulary_size = 22500        \n",
        "    embedding = read_embeddings(vocabulary_size)\n",
        "    '''\n",
        "    #Fuentes: See if the protagonists appear in the embedding list    \n",
        "    #Fuentes: I recommend increasing vocabulary size until all 3 appear in vocabulary\n",
        "    for w in ['buck','dracula','holmes']:\n",
        "        try:\n",
        "            print(w,'embedding:\\n',embedding[w])\n",
        "        except:\n",
        "            print(w,'is not in dictionary')\n",
        "            pass\n",
        "    '''\n",
        "    \n",
        "    #Regarding y > Each sentence is mapped to a book > 2D array mapped to a number(1-3)\n",
        "    \n",
        "    #get each word embedding from each book - contains duplicated embeddings\n",
        "    desiredLength = 7\n",
        "    all_word_emb = [] #should be a list of 2D arrays where each 2D array is a sentence embedding (represents the Word2vec)\n",
        "    for sent in allSentences:\n",
        "        all_word_emb.append(sent_embedder(sent, embedding,desiredLength))\n",
        "\n",
        "    print(\"Total word embeddings calculated: \",len(all_word_emb)) #should be 12260 bc thats how many sentences are in all 3 books combined -\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Book 0 contains 1618 sentences\n",
            "Sentence length stats (min,max and mean words in a sentence):\n",
            "min = 1 max = 122 mean = 19.241656\n",
            "Book 1 contains 4219 sentences\n",
            "Sentence length stats (min,max and mean words in a sentence):\n",
            "min = 1 max = 125 mean = 17.345105\n",
            "Book 2 contains 6423 sentences\n",
            "Sentence length stats (min,max and mean words in a sentence):\n",
            "min = 1 max = 101 mean = 15.205511\n",
            "Total number of sentences in all 3 books:  12260\n",
            "Total word embeddings calculated:  12260\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVPCAi-p-mPZ"
      },
      "source": [
        "Seperate the data into X and y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ziat2J9z-sqp"
      },
      "source": [
        "X = all_word_emb\n",
        "y = [] # create a one hot rep of the data | [1,0,0] means book 0, [0,1,0] means book 1, [0,0,1] means book 2\n",
        "\n",
        "#numSentEachBook is a list where index 0(book zero) contains number of sentences for that book and so on\n",
        "for book in range(len(numSentEachBook)):\n",
        "  for sent in range(numSentEachBook[book]):\n",
        "    ohRep = np.zeros(3) #3 bc there are 3 classes | book 0,1,2\n",
        "    ohRep[book] = 1\n",
        "    y.append(ohRep)\n",
        "\n",
        "#print(len(X)) # used for debugging\n",
        "#print(len(y)) # used for debugging"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd7ggpW7PWB2"
      },
      "source": [
        "Split into training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dIyyUucPZRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9fac68f-c1ef-4cad-8b6d-94fd6a4459db"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(X.shape)\n",
        "print(type(X))\n",
        "print(y.shape)\n",
        "print(type(y))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2020)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(type(X_train))\n",
        "print(y_train.shape)\n",
        "print(type(y_train))\n",
        "\n",
        "print(X_test.shape)\n",
        "print(type(X_test))\n",
        "print(y_test.shape)\n",
        "print(type(y_test))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12260, 7, 50)\n",
            "<class 'numpy.ndarray'>\n",
            "(12260, 3)\n",
            "<class 'numpy.ndarray'>\n",
            "(9195, 7, 50)\n",
            "<class 'numpy.ndarray'>\n",
            "(9195, 3)\n",
            "<class 'numpy.ndarray'>\n",
            "(3065, 7, 50)\n",
            "<class 'numpy.ndarray'>\n",
            "(3065, 3)\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXc-GcRRQfAx"
      },
      "source": [
        "Use a CNN to classify what book a given sentence belongs to"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w4kbOTiR1jy"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, SGD\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import distutils\n",
        "\n",
        "'''\n",
        "tf.keras.layers.Conv1D(\n",
        "    filters,\n",
        "    kernel_size,\n",
        "    strides=1,\n",
        "    padding=\"valid\",\n",
        "    data_format=\"channels_last\",\n",
        "    dilation_rate=1,\n",
        "    groups=1,\n",
        "    activation=None,\n",
        "    use_bias=True,\n",
        "    kernel_initializer=\"glorot_uniform\",\n",
        "    bias_initializer=\"zeros\",\n",
        "    kernel_regularizer=None,\n",
        "    bias_regularizer=None,\n",
        "    activity_regularizer=None,\n",
        "    kernel_constraint=None,\n",
        "    bias_constraint=None,\n",
        "    **kwargs\n",
        ")\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "def create_model():\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Conv1D(32, 3, input_shape=(X_train.shape[1],X_train.shape[2]), activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
        "\n",
        "  model.add(tf.keras.layers.Dropout(.2))\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Conv1D(64, 3, input_shape=(X_train.shape[1],X_train.shape[2]), activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
        "\n",
        "\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(64,activation ='relu'))\n",
        "  model.add(tf.keras.layers.Dense(3,activation = 'softmax'))\n",
        "  return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI3hhs3FR_LA",
        "outputId": "d12c98ae-2442-4e45-dc60-4a797d27b43d"
      },
      "source": [
        "model = create_model()\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[tf.keras.metrics.])\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=50, epochs=20, validation_data=(X_test, y_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_3 (Conv1D)            (None, 5, 64)             9664      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 2, 64)             0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 18,115\n",
            "Trainable params: 18,115\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "184/184 [==============================] - 1s 4ms/step - loss: 0.9122 - accuracy: 0.5257 - val_loss: 0.8805 - val_accuracy: 0.5419\n",
            "Epoch 2/20\n",
            "184/184 [==============================] - 1s 3ms/step - loss: 0.8289 - accuracy: 0.5842 - val_loss: 0.8359 - val_accuracy: 0.5729\n",
            "Epoch 3/20\n",
            "184/184 [==============================] - 1s 3ms/step - loss: 0.7773 - accuracy: 0.6240 - val_loss: 0.8475 - val_accuracy: 0.5788\n",
            "Epoch 4/20\n",
            "184/184 [==============================] - 1s 3ms/step - loss: 0.7374 - accuracy: 0.6492 - val_loss: 0.8182 - val_accuracy: 0.5791\n",
            "Epoch 5/20\n",
            "184/184 [==============================] - 1s 3ms/step - loss: 0.7016 - accuracy: 0.6660 - val_loss: 0.8091 - val_accuracy: 0.5902\n",
            "Epoch 6/20\n",
            "184/184 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.6920 - val_loss: 0.8265 - val_accuracy: 0.5840\n",
            "Epoch 7/20\n",
            "184/184 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.7115 - val_loss: 0.8311 - val_accuracy: 0.5948\n",
            "Epoch 8/20\n",
            "184/184 [==============================] - 1s 3ms/step - loss: 0.6002 - accuracy: 0.7325 - val_loss: 0.8347 - val_accuracy: 0.5889\n",
            "Epoch 9/20\n",
            "184/184 [==============================] - 1s 3ms/step - loss: 0.5761 - accuracy: 0.7405 - val_loss: 0.8436 - val_accuracy: 0.6010\n",
            "Epoch 10/20\n",
            "184/184 [==============================] - 1s 4ms/step - loss: 0.5434 - accuracy: 0.7644 - val_loss: 0.8583 - val_accuracy: 0.5961\n",
            "Epoch 11/20\n",
            "184/184 [==============================] - 1s 3ms/step - loss: 0.5078 - accuracy: 0.7837 - val_loss: 0.8807 - val_accuracy: 0.5993\n",
            "Epoch 12/20\n",
            "184/184 [==============================] - 1s 3ms/step - loss: 0.4892 - accuracy: 0.7885 - val_loss: 0.8943 - val_accuracy: 0.5938\n",
            "Epoch 13/20\n",
            "184/184 [==============================] - 1s 3ms/step - loss: 0.4588 - accuracy: 0.8042 - val_loss: 0.9497 - val_accuracy: 0.5873\n",
            "Epoch 14/20\n",
            "184/184 [==============================] - 1s 4ms/step - loss: 0.4371 - accuracy: 0.8152 - val_loss: 0.9963 - val_accuracy: 0.6072\n",
            "Epoch 15/20\n",
            "184/184 [==============================] - 1s 3ms/step - loss: 0.4130 - accuracy: 0.8274 - val_loss: 0.9661 - val_accuracy: 0.6039\n",
            "Epoch 16/20\n",
            "184/184 [==============================] - 1s 3ms/step - loss: 0.3928 - accuracy: 0.8372 - val_loss: 1.0024 - val_accuracy: 0.5951\n",
            "Epoch 17/20\n",
            "184/184 [==============================] - 1s 4ms/step - loss: 0.3663 - accuracy: 0.8521 - val_loss: 1.0416 - val_accuracy: 0.5954\n",
            "Epoch 18/20\n",
            "184/184 [==============================] - 1s 3ms/step - loss: 0.3534 - accuracy: 0.8583 - val_loss: 1.0822 - val_accuracy: 0.6029\n",
            "Epoch 19/20\n",
            "184/184 [==============================] - 1s 4ms/step - loss: 0.3266 - accuracy: 0.8724 - val_loss: 1.0941 - val_accuracy: 0.6013\n",
            "Epoch 20/20\n",
            "184/184 [==============================] - 1s 4ms/step - loss: 0.3065 - accuracy: 0.8832 - val_loss: 1.1428 - val_accuracy: 0.6082\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}